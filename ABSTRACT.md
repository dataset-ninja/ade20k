The authors of the **ADE20K** dataset address the significant challenge of scene parsing, encompassing the recognition and segmentation of objects and stuff within images, a vital task in the domain of computer vision. Despite the efforts made by the research community to gather data, there remains a scarcity of image datasets that comprehensively cover a broad spectrum of scenes and object categories, along with detailed and dense annotations suitable for scene parsing. To fill this void, the authors introduce the ADE20K dataset. This dataset features diverse annotations that span **scenes**, **objects**, **parts** of objects, and, intriguingly, even **parts of parts**. In order to facilitate benchmarking for scene parsing, the ADE20K dataset includes 150 object and stuff classes, and various segmentation baseline models undergo evaluation using this benchmark. You can access the hierarchy of classes on the [official website of the dataset](https://groups.csail.mit.edu/vision/datasets/ADE20K/).

In the realm of computer vision, gaining a semantic understanding of visual scenes is a paramount goal. Contemporary robotic systems can accurately predict scene categories from visual scenes such as living rooms. However, for seamless navigation and manipulation within scenes, robots must digest an extensive amount of information. This entails recognizing and localizing not only primary objects like sofas, tables, and TVs but also their parts, such as the seat of a chair or the handle of a cup. Furthermore, these systems need to segment elements like floors, walls, and ceilings to facilitate spatial navigation.

Recognizing and segmenting objects and stuff within images remains a pivotal problem in scene understanding. Beyond the domain of image-level recognition, scene parsing necessitates denser scene annotations encompassing a broad spectrum of objects. However, current datasets often possess a limited count of objects, and some objects may not represent the most common elements encountered in the real world. Moreover, the datasets are confined to specific scene categories.

The dataset summary indicates that the training set comprises 20,210 images, the validation set contains 2,000 images, and the testing set contains 3,000 images. All images undergo exhaustive annotations for objects, with many objects also annotated for their respective parts. Each object includes additional information about attributes like occlusion or cropping. While the validation set is meticulously annotated with parts, the annotations for parts are not exhaustive across images in the training set. The annotations within the dataset are continually expanding, reflecting ongoing efforts.

The process of image annotation involved sourcing images from datasets such as [LabelMe](https://people.csail.mit.edu/brussell/research/AIM-2005-025-new.pdf), [SUN](https://ieeexplore.ieee.org/document/5539970), and [Places](http://places.csail.mit.edu/places_NIPS14.pdf), with a focus on capturing a diverse set of scenes that cover the 900 ***scene*** categories delineated in the SUN database. These images were annotated by a <i>single</i> dedicated expert worker using the LabelMe interface. Annotations were divided into three types: object segments with associated names, object parts, and attributes. Notably, all object instances underwent independent segmentation to enable the dataset's use for training and evaluating detection or segmentation algorithms. Unlike datasets such as COCO, Pascal, or Cityscape which begin by defining a predefined set of object categories, <i>this dataset embraces the challenge of labeling all objects within a scene</i>, which frequently results in the emergence of new object categories. To manage this dynamic scenario, the annotator curated a dictionary of visual concepts, consistently adding new classes to maintain object naming coherence.

<img src="https://github.com/dataset-ninja/gland-segmentation/assets/78355358/581dfa9b-cf25-474d-a79e-090f45c4a471" alt="image" width="800">

<span style="font-size: smaller; font-style: italic;">Annotation interface, the list of the objects and their associated parts in the image.</span>

The authors also addressed the associations of object parts, which might themselves have parts, contributing to a hierarchical structure. This hierarchy reaches a depth of 3 and is elaborated upon in supplementary materials. For instance, a *door* can be an object (in an indoor picture), or a part (when it is the *door* of a *car*). Some objects are always parts (e.g., a *leg*, a *hand*, ...), although, in some cases, they can appear detached from the whole (e.g., a car *wheel* inside a garage), and some objects are never parts (e.g., a *person*, a *truck*, ...). The same name class (e.g., *door*) can correspond to several visual categories depending on which object it is a part of. For instance, a car door is visually different from a cabinet door or a building door. However, they share similar affordances. The value **proportionClassIsPart(c)** can be used to decide if a class behaves mostly as an object or as a part. When an object is not part of another object its segmentation mask will appear inside *_seg.png. If the class behaves as a part, then the segmentation mask will appear inside *_seg_parts.png. Correctly detecting an object requires classifying if the object is behaving as an independent object or if it is a part of another object.

<img src="https://github.com/dataset-ninja/gland-segmentation/assets/78355358/2bc8d740-fbcb-4964-9e81-3279f441ea70" alt="image" width="800">

An in-depth analysis of annotation consistency underscores the complexities arising when class lists are open-ended. While labeling tasks with a fixed list of object classes is relatively straightforward, challenges emerge when the list of classes grows unbounded due to new categories appearing over time. The authors conducted an assessment by re-annotating a subset of images from the validation set after a six-month interval. Notably, 82.4% of the pixels retained the same label across both annotations. The remaining 17.6% of pixels exhibited errors, which were classified into three error types: segmentation quality, object naming, and segmentation quantity. The median error rates for these error types were 4.8%, 0.3%, and 2.6%, respectively, indicating that the mean error rate was influenced by a few images. The most common error type was identified as segmentation quality.

<img src="https://github.com/dataset-ninja/gland-segmentation/assets/78355358/6993d915-3846-4904-8482-11a34ef86cee" alt="image" width="800">

<span style="font-size: smaller; font-style: italic;">(a) Object classes are sorted by frequency. Only the top 270 classes with more than 100 annotated instances are shown. 68 classes have more than 1000 segmented instances. (b) Frequency of parts grouped by objects. There are more than 200 object classes with annotated parts. Only objects with 5 or more parts are shown in this plot (we show at most 7 parts for each object class). \(c\) Objects ranked by the  number of scenes they are part of. (d) Object parts are ranked by the number of objects they are part of. (e) Examples of objects with doors. The bottom-right image is an example where the door does not behave as a part.</span>

Moreover, dataset statistics reveal the distribution of ranked object frequencies, closely resembling Zipf's law, which is often observed when objects are comprehensively annotated. Distributions of annotated parts, object sharing across scenes, parts shared by objects, and the variability in part appearances were also examined. Additional analyses demonstrated the mode of object segmentations, distributions of distinct classes and instances, and distributions of parts within the dataset. In particular, the authors highlighted the emergence of new object classes as annotation progresses, quantifying the probability of encountering a new class after labeling a certain number of instances.
